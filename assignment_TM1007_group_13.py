# -*- coding: utf-8 -*-
"""assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X29Ut2niBQJjG7_JSFaFBGIEDF_br8Or

# TM10007 Assignment

---
"""

# Run this to use from colab environment
!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git

"""Import modules"""

# Data loading function
from brats.load_data import load_data

# General packages
import numpy as np
import pandas
import matplotlib.pyplot as plt

# Preprocessing
from sklearn.preprocessing import RobustScaler
from sklearn import model_selection
from sklearn import preprocessing
from sklearn import metrics
from sklearn.impute import SimpleImputer
from sklearn.impute import KNNImputer
from sklearn import model_selection

# Classifiers and kernels
from sklearn import neighbors
from scipy import stats
from sklearn.decomposition import PCA
from sklearn import svm
from sklearn import feature_selection 
from sklearn import model_selection
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA, KernelPCA
from sklearn.kernel_approximation import RBFSampler
from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit
from sklearn import metrics
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# Regularization
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from time import time

# Functions for plotting ROC curve
from sklearn.metrics import roc_curve, auc
from scipy import interp
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import label_binarize

"""Functions"""

#@title Default title text

# Preprocessing: fill missing data
def missing_data(X_train,Y_train):
    # Fill by KNN Imputer
    imputer_KNN = KNNImputer(n_neighbors=2, weights="uniform")
    X_train_KNN_imputed = imputer_KNN.fit_transform(X_train)
    X_test_KNN_imputed = imputer_KNN.fit_transform(X_test)
    return X_train_KNN_imputed,X_test_KNN_imputed 

# Preprocessing: feature scaling (standard scaling)
def standard_scaler(X_train_KNN_imputed, X_test_KNN_imputed):
    # Set standard scaler
    scaler = preprocessing.StandardScaler()
    scaler.fit(X_train_KNN_imputed)
    X_train_scaled = scaler.transform(X_train_KNN_imputed)
    X_test_scaled = scaler.transform(X_test_KNN_imputed)
    return X_train_scaled, X_test_scaled

def split_data(data, data_label):
    # Split the dataset in train (70%) and test part (30%)
    X_train, X_test, y_train, y_test = model_selection.train_test_split(data, data_label, test_size=0.3, stratify=(data_label))

    # Give new indexes to samples
    X_train = X_train.reset_index()
    X_train = X_train.drop(columns=['index'])
    X_test = X_test.reset_index()
    X_test = X_test.drop(columns=['index'])
  
    # Remove the label columns from the train and the test data
    X_train = X_train.drop(columns=["label"])
    X_test = X_test.drop(columns=["label"])

    # Print number of samples in both sets
    print(f'The number of samples in the training set: {len(X_train.index)}')
    print(f'The number of samples in the test set: {len(X_test.index)}')
    return(X_train, X_test, y_train, y_test)
    
# Feature selection: PCA
def pca(X_train_scaled, X_test_scaled):
    # Train and plot Principal Component Analysis
    
    pca = PCA().fit(X_train_scaled)
    fig2, ax2 = plt.subplots()
    ax2.plot(np.cumsum(pca.explained_variance_ratio_))
    ax2.set_title('Prinicpal Component Analysis')
    ax2.set_xlabel('Number of components')
    ax2.set_ylabel('Cumulative explained variance')

    # Apply PCA to keep 98% of the variance = 70 components
    pca = PCA(n_components=70)
    pca.fit(X_train_scaled)
    X_train_pca = pca.transform(X_train_scaled)
    X_test_pca = pca.transform(X_test_scaled)
    return X_train_pca, X_test_pca

# Plot ROC curve
def plot_roc_curve(y_score, y_truth):
    '''
    Plot an ROC curve.
    '''
    # Only take scores for class = 1
    y_score = y_score[:, 1]
    
    # Compute ROC curve and ROC area for each class
    fpr, tpr, _ = roc_curve(y_truth, y_score)
    roc_auc = auc(fpr, tpr)
    
    # Plot the ROC curve
    plt.figure()
    lw = 2
    plt.plot(fpr, tpr, color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()
    return roc_auc

# Feature selection: recursive feature elimination
def rfecv_featureselection(X_train, X_test, y_train):
    from sklearn import feature_selection 

    # Create the SVC classifier
    svc = svm.SVC(kernel="linear")

    # Recursive feature elimination
    rfecv = feature_selection.RFECV(estimator=svc, step=1, 
                                cv=model_selection.StratifiedKFold(10), 
                                scoring='roc_auc')
    rfecv.fit(X_train, y_train)
    X_train_rfecv = rfecv.transform(X_train)
    X_test_rfecv = rfecv.transform(X_test)

    # Plot number of features VS. cross-validation scores
    fig2, ax1 = plt.subplots()
    ax1.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
    ax1.set_title('Recursive feature elimination')
    ax1.set_xlabel("Number of features selected")
    ax1.set_ylabel("Cross validation score (nb of correct classifications)")
    print(X_train_rfecv.shape)
    return X_train_rfecv, X_test_rfecv

# L1 (LASSO) Regularization
def regularization_lasso(X_train, X_test, y_train, y_test):
    # Display the weights and compute error for multiple values for alpha
    n_alphas = 200
    alphas = np.logspace(-10, 1, n_alphas)
    # Construct classifiers
    coefs = []
    accuracies = []
    times = []
    for a in alphas:
        # Fit classifier
        clf = Lasso(alpha=a, fit_intercept=False)
        t0 = time()
        clf.fit(X_train, y_train)
        duration = time() - t0
        y_pred = clf.predict(X_test)
        message = ("\t Misclassified: %d / %d" % ((y_test != y_pred).sum(), y_test.shape[0]))
        print(message)
        
        # Append statistics
        accuracy = float((y_test != y_pred).sum()) / float(y_test.shape[0])
        times.append(duration)
        accuracies.append(accuracy)
        coefs.append(clf.coef_)

    # #############################################################################
    # Display results

    # Weights
    plt.figure()
    ax = plt.gca()
    ax.plot(alphas, np.squeeze(coefs))
    ax.set_xscale('log')
    ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
    plt.xlabel('alpha')
    plt.ylabel('weights')
    plt.title('Lasso coefficients as a function of the regularization')
    plt.axis('tight')
    plt.show()

    # Performance
    plt.figure()
    ax = plt.gca()
    ax.plot(alphas, accuracies)
    ax.set_xscale('log')
    ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
    plt.xlabel('alpha')
    plt.ylabel('accuracies')
    plt.title('Performance as a function of the regularization')
    plt.axis('tight')
    plt.show()

    # Times
    plt.figure()
    ax = plt.gca()
    ax.plot(alphas, times)
    ax.set_xscale('log')
    ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
    plt.xlabel('alpha')
    plt.ylabel('times (s)')
    plt.title('Fitting time as a function of the regularization')
    plt.axis('tight')
    plt.show()

# Select from module LASSO
def select_from_module_lasso(X_train, X_test, y_train):
    selector = SelectFromModel(estimator=Lasso(alpha=10**(-8),random_state = 42 ), threshold='median') #random_state = 42
    selector.fit(X_train, y_train)
    n_original = X_train.shape[1]
    X_train = selector.transform(X_train)
    X_test = selector.transform(X_test)
    n_selected = X_train.shape[1]
    print(f"Selected {n_selected} from {n_original} features.")
    return(X_train, X_test)

# Optimization hyperparameters KNN classifier
def optimalisation_knn(X_train, X_test, y_train, y_test):
    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation

    # Specify the classifier
    knn = neighbors.KNeighborsClassifier()

    # Specify the search range, this could be multiple parameters for more complex classifiers
    parameters = {
        "n_neighbors": list(range(1, 8, 2))
    }


    # Create the grid search method, use area under ROC curve as scoring metric
    # Too learn more about metrics see: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
    grid_search = RandomizedSearchCV(knn, parameters, cv=10, scoring='roc_auc', random_state =42)

    # Do the entire search
    grid_search.fit(X_train, y_train)

    clf_knn = grid_search.best_estimator_
    accuracy = clf_knn.score(X_test,y_test)
    y_score = clf_knn.predict_proba(X_test)
    plot_roc_curve(y_score, y_test)

    # Only take scores for class = 1
    y_score = y_score[:, 1]
    
    # Compute ROC curve and ROC area for each class
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    return accuracy, roc_auc

# Optimization SVM
def optimalisatie_svm(X_train,X_test,y_train, y_test):

    Cs = [0.001, 0.01, 0.1, 1, 10]
    param_distributions = {'C': Cs}
    clf = RandomizedSearchCV(SVC(kernel='rbf'), param_distributions, cv=10, random_state=42) #random_state=42 
    
    # Fit the classifier
    clf.fit(X_train, y_train)
    accuracy = clf.score(X_test,y_test)
    y_score = clf.predict(X_test)
    
    roc_auc = metrics.roc_auc_score(y_test, y_score)
    return accuracy, roc_auc

# L1 Regularization (LASSO)
def lasso(X_train,X_test,y_train, y_test):
          
    X_train_lasso, X_test_lasso = select_from_module_lasso(X_train, X_test, y_train)

    # Fit LDA on selected features (LASSO)
    clf = LDA()
    clf.fit(X_train_lasso, y_train)
    accuracy = clf.score(X_test_lasso,y_test)
    y_score = clf.predict_proba(X_test_lasso)[:,1]
    roc_auc=metrics.roc_auc_score(y_test, y_score)
  
    return accuracy, roc_auc

# Plot learning curves
def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    """
    Generate 3 plots: the test and training learning curve, the training
    samples vs fit times curve, the fit times vs score curve.

    Parameters
    ----------
    estimator : object type that implements the "fit" and "predict" methods
        An object of that type which is cloned for each validation.

    title : string
        Title for the chart.

    X : array-like, shape (n_samples, n_features)
        Training vector, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape (n_samples) or (n_samples, n_features), optional
        Target relative to X for classification or regression;
        None for unsupervised learning.

    axes : array of 3 axes, optional (default=None)
        Axes to use for plotting the curves.

    ylim : tuple, shape (ymin, ymax), optional
        Defines minimum and maximum yvalues plotted.

    cv : int, cross-validation generator or an iterable, optional
        Determines the cross-validation splitting strategy.
        Possible inputs for cv are:
          - None, to use the default 5-fold cross-validation,
          - integer, to specify the number of folds.
          - :term:`CV splitter`,
          - An iterable yielding (train, test) splits as arrays of indices.

        For integer/None inputs, if ``y`` is binary or multiclass,
        :class:`StratifiedKFold` used. If the estimator is not a classifier
        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.

        Refer :ref:`User Guide <cross_validation>` for the various
        cross-validators that can be used here.

    n_jobs : int or None, optional (default=None)
        Number of jobs to run in parallel.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.

    train_sizes : array-like, shape (n_ticks,), dtype float or int
        Relative or absolute numbers of training examples that will be used to
        generate the learning curve. If the dtype is float, it is regarded as a
        fraction of the maximum size of the training set (that is determined
        by the selected validation method), i.e. it has to be within (0, 1].
        Otherwise it is interpreted as absolute sizes of the training sets.
        Note that for classification the number of samples usually have to
        be big enough to contain at least one sample from each class.
        (default: np.linspace(0.1, 1.0, 5))
    """

    axes.set_title(title)
    if ylim is not None:
        axes.set_ylim(*ylim)
    axes.set_xlabel("Training examples")
    axes.set_ylabel("Score")

    train_sizes, train_scores, test_scores  = \
        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
                       train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    # Plot learning curve
    axes.grid()
    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1,
                         color="r")
    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1,
                         color="g")
    axes.plot(train_sizes, train_scores_mean, 'o-', color="r",
                 label="Training score")
    axes.plot(train_sizes, test_scores_mean, 'o-', color="g",
                 label="Cross-validation score")
    axes.legend(loc="best")

    return plt

#learning curves 2
def learning_curve_two(X_train, X_test, y_train, y_test):

    # Define possible classifiers
    clsfs = [RandomForestClassifier(n_estimators=1, random_state = 42), #random_state = 42
             RandomForestClassifier(n_estimators=5,random_state = 42), #random_state = 42
             RandomForestClassifier(n_estimators=10,random_state = 42), #random_state = 42
             KNeighborsClassifier(n_neighbors=1),
             SVC(kernel='linear'),
             SVC(kernel='rbf', gamma='scale'), 
             SVC(kernel='poly', degree=3, gamma='scale'),
             SVC(kernel='rbf', gamma='scale',  C = 0.01),
             SVC(kernel='rbf', gamma='scale',  C= 1),
             KNeighborsClassifier(n_neighbors=4),
             KNeighborsClassifier(n_neighbors=8),
             SVC(kernel='rbf', gamma='scale',  C= 0.5),]
            

    # Cross Validation
    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42) #random_state=42
    fig = plt.figure(figsize=(24,8))
    num = 0

    # Create learning curves
    for clf in clsfs:
        title = str(type(clf))
        ax = fig.add_subplot(4, 3, num + 1)
        plot_learning_curve(clf, title, X_train_pca, y_train, ax, ylim=(0.3, 1.01), cv=cv)
        num += 1

"""Load and adjust data"""

# Load data
data = load_data()

# Adjust data (replace infinity, #DIV/0 and 0 by NaN)
data = data.replace([np.inf, -np.inf], np.nan)
data = data.replace("#DIV/0!", np.nan)
data = data.replace(0, np.nan)

# Set outcome GBM=1 and LGG=0
data = data.replace("GBM", 1)
data = data.replace("LGG", 0)

# Remove ID index and replace by new index increasing from 1 to N
data = data.reset_index()
data = data.drop(columns=["ID"])

print(data)

# Print number of samples and columns (=number of features + 1)
print(f'The number of samples: {len(data.index)}')
print(f'The number of columns: {len(data.columns)}')

"""Outer loop"""

cv_5fold = model_selection.StratifiedKFold(n_splits=5)

results_accuracy_knn_pca = []
results_accuracy_knn_rfecv = []
results_accuracy_svm_pca = []
results_accuracy_svm_rfecv = []
results_accuracy_L1 = []

results_auc_knn_pca = []
results_auc_knn_rfecv = []
results_auc_svm_pca = []
results_auc_svm_rfecv = []
results_auc_L1 = []

x = data.drop(columns = ["label"])
y = data.label

# Feature selection
# Search for features with >60% missing data (NaN values)
missing = x.isnull().sum(axis = 0).reset_index()
missing.columns = ['column_name', 'missing_count']
missing = missing.loc[missing['missing_count']>(0.6*len(x.index))] 
print(f'The number of features that have more than 60% missing data is {missing.shape[0]}. The following features are removed:')
print(missing)

# Remove features >60% missing data (NaN values)
x = x.drop(missing['column_name'], axis=1, inplace=False)
print(x.shape)
    
x = x.to_numpy()
y = y.to_numpy()

for train_index, test_index in cv_5fold.split(x, y):
    
    # Create train and test feature matrices and label vectors
    X_train = x[train_index]
    y_train = y[train_index]
    X_test = x[test_index]
    y_test = y[test_index]

    # Perform KNN-imputing on train and test set
    [X_train_KNN_imputed, X_test_KNN_imputed] = missing_data(X_train,y_train)
    # Perform scaling on train and test set
    [X_train_scaled, X_test_scaled] = standard_scaler(X_train_KNN_imputed, X_test_KNN_imputed)

    # L1 Regularization (LASSO)
    regularization_lasso(X_train_scaled, X_test_scaled, y_train, y_test)
    X_train_lasso, X_test_lasso = select_from_module_lasso(X_train_scaled, X_test_scaled, y_train)
    # Fit LDA on selected features (LASSO)
    clf = LDA()
    clf.fit(X_train_lasso, y_train)
    y_score = clf.predict_proba(X_test_lasso)
    plot_roc_curve(y_score, y_test)

    # Perform PCA on train and test set
    [X_train_pca, X_test_pca] = pca(X_train_scaled, X_test_scaled)
    # Perform RFECV on train and test set
    [X_train_rfecv, X_test_rfecv] = rfecv_featureselection(X_train_scaled, X_test_scaled, y_train)

    # Create learning Curves
    learning_curve_two(X_train_pca, X_test_pca, y_train, y_test)
    learning_curve_two(X_train_rfecv, X_test_rfecv, y_train, y_test)

    # Calculate accuracies for all combinations of classifiers and selection methods
    [accuracy_knn_pca, auc_knn_pca] = optimalisation_knn(X_train_pca, X_test_pca, y_train, y_test)
    results_accuracy_knn_pca.append(accuracy_knn_pca)
    results_auc_knn_pca.append(auc_knn_pca)
    [accuracy_knn_rfecv, auc_knn_rfecv] = optimalisation_knn(X_train_rfecv, X_test_rfecv, y_train, y_test)
    results_accuracy_knn_rfecv.append(accuracy_knn_rfecv)
    results_auc_knn_rfecv.append(auc_knn_rfecv)
    [accuracy_svm_pca, auc_svm_pca] = optimalisatie_svm(X_train_pca,X_test_pca,y_train, y_test)
    results_accuracy_svm_pca.append(accuracy_svm_pca)
    results_auc_svm_pca.append(auc_svm_pca)
    [accuracy_svm_rfecv, auc_svm_rfecv] = optimalisatie_svm(X_train_rfecv,X_test_rfecv,y_train, y_test)
    results_accuracy_svm_rfecv.append(accuracy_svm_rfecv)
    results_auc_svm_rfecv.append(auc_svm_rfecv)
    [accuracy_L1, auc_L1] = lasso(X_train_scaled,X_test_scaled,y_train, y_test)
    results_accuracy_L1.append(accuracy_L1)
    results_auc_L1.append(auc_L1)

# Show all accuracies of classifiers and selection methods
print(results_accuracy_knn_pca)
print(results_accuracy_svm_pca)
print(results_accuracy_knn_rfecv)
print(results_accuracy_svm_rfecv)
print(results_accuracy_L1)

# Calculate mean accuracies for each classifier and selection method
final_accuracy_knn_pca = np.mean(results_accuracy_knn_pca)
final_auc_knn_pca = np.mean(results_auc_knn_pca)
print('accuracy knn and pca is {}, auc knn and pca is {}' .format(final_accuracy_knn_pca,final_auc_knn_pca ))

final_accuracy_svm_pca = np.mean(results_accuracy_svm_pca)
final_auc_svm_pca = np.mean(results_auc_svm_pca)
print('accuracy svm and pca is {}, auc svm and pca is {}'.format(final_accuracy_svm_pca, final_auc_svm_pca))

final_accuracy_knn_rfecv = np.mean(results_accuracy_knn_rfecv)
final_auc_knn_rfecv = np.mean(results_auc_knn_rfecv)
print('accuracy knn and rfecv is {}, auc knn and rfecv is {}'.format(final_accuracy_knn_rfecv, final_auc_knn_rfecv))

final_accuracy_svm_rfecv = np.mean(results_accuracy_svm_rfecv)
final_auc_svm_rfecv = np.mean(results_auc_svm_rfecv)
print('accuracy svm and rfecv is {}, auc svm and rfecv is {}'.format(final_accuracy_svm_rfecv, final_auc_svm_rfecv))

final_accuracy_L1 = np.mean(results_accuracy_L1)
final_auc_L1 = np.mean(results_auc_L1)
print('accuracy L1 is {}, auc L1 is {}'.format(final_accuracy_L1, final_auc_L1))

# Choose the best performing classifier and selection method
mean_accuracies = [np.mean(results_accuracy_knn_pca), np.mean(results_accuracy_svm_pca), np.mean(results_accuracy_knn_rfecv), np.mean(results_accuracy_svm_rfecv), np.mean(results_accuracy_L1)]
max_accuracy_index = mean_accuracies.index(max(mean_accuracies))
classifiers = ['knn and pca', 'svm and pca', 'knn and rfecv', 'svm and rfecv', 'L1 and LDA']
print(f'The best performing combination of classifier and selection method is {classifiers[max_accuracy_index]} with an accuracy of {mean_accuracies[max_accuracy_index]}.')